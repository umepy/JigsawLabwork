{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import datasets\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "from chainer.backends import cuda\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jigsaw_dataset(chainer.dataset.DatasetMixin):\n",
    "    def __init__(self, train=True, min_count=10, train_size=0.6):\n",
    "        train_df = pd.read_csv(\"../input/train.csv\")\n",
    "        \n",
    "        t = []\n",
    "        for s in train_df['comment_text']:\n",
    "            t.append(s.split())\n",
    "                \n",
    "        words = {}\n",
    "        for j in range(len(t)):\n",
    "            for word in t[j]:\n",
    "                if word not in words:\n",
    "                    words[word] = len(words)\n",
    "        \n",
    "        t_vec = []\n",
    "        for j in range(len(t)):\n",
    "            t_ids = []\n",
    "            for word in t[j]:\n",
    "                t_ids.append(words[word])\n",
    "            t_ids = np.array(t_ids, dtype=np.int32)\n",
    "            t_vec.append(t_ids)\n",
    "        \n",
    "        max_len = max(list(map(len, t_vec)))\n",
    "        for i in range(len(t_vec)):\n",
    "            for j in range((max_len-len(t_vec[i]))):\n",
    "                t_vec[i] = np.append(t_vec[i], -1)\n",
    "        \n",
    "        x = np.array(t_vec, dtype=np.int32)\n",
    "        \n",
    "        y= np.where(train_df['target']>0.5, 1, 0)\n",
    "#         label = pd.get_dummies(label)\n",
    "#         y = label.values\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(x, y, train_size=self.train_size, random_state=1)\n",
    "        self.n_train = len(self.y_train)\n",
    "        self.n_test = len(self.y_test)\n",
    "        self.train = train\n",
    "        self.train_size=train_size\n",
    "        \n",
    "        del t, words, t_vec, t_ids, x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.n_train\n",
    "        else:\n",
    "            return self.n_test\n",
    "    \n",
    "    def get_example(self, i):\n",
    "        \n",
    "        train = datasets.tuple_dataset.TupleDataset(self.X_train, self.y_train)\n",
    "        test = datasets.tuple_dataset.TupleDataset(self.X_test, self.y_test)\n",
    "        \n",
    "        if self.train:\n",
    "            return train[i]\n",
    "        else:\n",
    "            return test[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(chainer.Chain):\n",
    "    def __init__(self, n_vocab=1670966, n_units=100):\n",
    "        super(RNN, self).__init__(\n",
    "            embed = L.EmbedID(n_vocab, n_units),\n",
    "            l1 = L.LSTM(n_units, n_units),\n",
    "            l2 = L.LSTM(n_units, n_units),\n",
    "            l3 = L.Linear(n_units, 2)\n",
    "        )\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0))\n",
    "        h2 = self.l2(F.dropout(h1))\n",
    "        y = F.softmax(self.l3(h2))\n",
    "        return y\n",
    "\n",
    "\n",
    "model = RNN()\n",
    "\n",
    "gpu_id = 0\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)\n",
    "    \n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "MAX_EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train = jigsaw_dataset(train_size=0.1)\n",
    "# test = jigsaw_dataset(train=False, train_size=0.9)\n",
    "# train_iter = chainer.iterators.SerialIterator(train, BATCH_SIZE)\n",
    "# test_iter = chainer.iterators.SerialIterator(test, BATCH_SIZE, repeat=False, shuffle=False)\n",
    "train_iter = chainer.iterators.SerialIterator(jigsaw_dataset(train_size=0.1), BATCH_SIZE)\n",
    "test_iter = chainer.iterators.SerialIterator(jigsaw_dataset(train=False, train_size=0.9), BATCH_SIZE, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while train_iter.epoch < MAX_EPOCH:\n",
    "#     train_batch = train_iter.next()\n",
    "#     sentence_train, target_train = chainer.dataset.concat_examples(train_batch)\n",
    "#     sentence_train, target_train = chainer.dataset.concat_examples(train_batch, gpu_id)\n",
    "    sentence_train, target_train = chainer.dataset.concat_examples(train_iter.next(), gpu_id)\n",
    "    \n",
    "    \n",
    "#     prediction_train = model(sentence_train)\n",
    "#     loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "    loss = F.softmax_cross_entropy(model(sentence_train), target_train)\n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update()\n",
    "    if train_iter.is_new_epoch:\n",
    "        print('epoch:{:02d} train_loss:{:.04f}'.format(train_iter.epoch, float(loss.array)), end='')\n",
    "        \n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "#             test_batch = test_iter.next()\n",
    "#             sentence_test, target_test = chainer.dataset.concat_examples(test_batch)\n",
    "#             sentence_test, target_test = chainer.dataset.concat_examples(test_batch, gpu_id)\n",
    "            sentence_test, target_test = chainer.dataset.concat_examples(test_iter.next(), gpu_id)\n",
    "            \n",
    "#             prediction_test = model(sentence_test)\n",
    "#             loss_test = F.mean_squared_error(prediction_test, target_test)\n",
    "            loss_test = F.mean_squared_error(model(sentence_test), target_test)\n",
    "            \n",
    "#             test_losses.append(loss_test.array)\n",
    "            test_losses.append(cuda.to_cpu(loss_test.array))\n",
    "            \n",
    "            accuracy = F.accuracy(prediction_test, target_test)\n",
    "            accuracy.cuda.to_cpu()#\n",
    "            test_accuracies.append(accuracy.array)\n",
    "            \n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch = 0\n",
    "                test_iter.current_position = 0\n",
    "                test_iter.is_new_epoch = False\n",
    "                test_iter._pushed_position = None\n",
    "                break;\n",
    "        \n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(np.mean(test_losses), np.mean(test_accuracies)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
