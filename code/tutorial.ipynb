{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Requirement\ngpu は on にして\n\n+add Dataset の内容\n- pytorch ignite 2.0\n- word2vec google\n- glove 840B 300d"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/pytorch-ignite-020/pytorch_ignite-0.2.0-py2.py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport gensim\nfrom ignite.contrib.handlers.tensorboard_logger import *\nfrom ignite.engine import Engine, Events\nfrom ignite.metrics import Accuracy, Loss, RunningAverage, Precision, Recall\nfrom ignite.handlers import ModelCheckpoint, EarlyStopping\nfrom ignite.contrib.handlers import ProgressBar\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport os\nimport pandas as pd\nimport pprint\nimport random\nimport re\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n%load_ext tensorboard.notebook\nimport tensorflow as tf\nimport time\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data.dataset import Subset\nimport torchtext\nfrom tqdm import tqdm_notebook as tqdm\n\n# .The などが分かれないので，とりあえず .と大文字に対する分割\nprog = re.compile('\\.[A-Z]')\n# 先頭大文字用\nHead_check = re.compile('^[A-Z][^A-Z]+$')\n# 乱数の固定\nSEED = 0\n# word2vec を学習するか\ntrain_word2vec = False\n# text の前処理をするか\ntext_preprocessing = True\n# tensorboad 用の path\nlog_path = os.path.join(\"experiments\", \"tb_logs\")\n# best model 用の path\nbest_model_path = \"best_model\"\n# MIN_COUNT に到達しない単語にも情報があれば付与したり，UNDEFINED などにしたほうが後々楽\nMIN_COUNT = 10\ndebug = True\ninput_dir = os.path.join(\"..\", \"input\")\njigsaw_path = \"jigsaw-unintended-bias-in-toxicity-classification\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"check cuda\", torch.cuda.is_available())\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if debug:\n    print(\"files in input directory\")\n    print(\"default path\", os.listdir())\n    print(input_dir, os.listdir(input_dir))\n    for path in os.listdir(input_dir):\n        print(os.path.join(input_dir, path), os.listdir(os.path.join(input_dir, path)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# disable progress bars when submitting らしいです．よく知らないです\n# kaggle 関係は別の人に聞いてください\ndef is_interactive():\n    #print(\"os environment\", 'SHLVL' not in os.environ)\n    return 'SHLVL' not in os.environ\n\n# 環境によって，tqdm が動作しないようですが，今回使ってないです\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed を固定しないと，実行結果が毎回変わってしまいます\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data の確認\n特に調べずに preprocessing しちゃっていますが，悪意のある単語というのはある程度決まっていると思うので，確認した方が良いと思います．\n\n例えばくらいですが\nhttps://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(input_dir, jigsaw_path, \"train.csv\"))\nif debug:\n    print(\"train columns\", train_df.columns)\n    print(\"train shape\", train_df.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(input_dir, jigsaw_path, \"test.csv\"))\nif debug:\n    print(\"test columns\", test_df.columns)\n    print(\"test shape\", test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if train_word2vec:\n    target = np.where(train_df[\"target\"] >= 0.5, True, False)\ntrain_shape = train_df.shape\nall_document = train_df[\"comment_text\"].tolist()+test_df[\"comment_text\"].tolist()\n# kaggle だとメモリ足らない\ndel train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list の混在がないかチェック\nif debug:\n    length = 2\n    print(all_document[:length])\n    print(all_document[train_shape[0]-length:train_shape[0]])\n    print(all_document[train_shape[0]:train_shape[0]+length])\n    print(all_document[-length:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text preprocessing\nstanfordnlp package が最も精度が良いとは思いますが，kaggle の kernel 上では厳しいなって感じです．\nローカルでやったのを上げて使えるようですが，詳しくないので．特に，単語間の依存関係は使えるかもしれません\n\nnltk や stanfordnlp のほかに，spacy や sentencepiece もあります．また，keras や torch にも組み込みのものはあるような．"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 先頭大文字への対処．固有名詞には基本触れない\nlemmatizer = nltk.stem.WordNetLemmatizer()\ndef word_compile(word, lemmatize = True):\n    # 先頭のみ大文字\n    check = Head_check.match(word)\n    if check is not None:\n        word = word[0].lower() + word[1:]\n    # 見出し語化\n    if lemmatize:\n        word = lemmatizer.lemmatize(word)\n    return word\n\nif debug:\n    print(word_compile(\"They\"))\n    print(word_compile(\"TRUMP\"))\n    print(word_compile(\"WordNet\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# .が繋がっているとわかれないので\ndef splitter(text, split_words, include = True):\n    split_words = [i.span()[0] for i in split_words]\n    word_list = []\n    word = \"\"\n    for i in range(len(text)):\n        word += text[i]\n        if i in split_words:\n            word_list.append(word_compile(word[:-1]))\n            if include:\n                word_list.append(\".\")\n            word = \"\"\n    word_list.append(word_compile(word))\n    return word_list\n\nif debug:\n    print(splitter(\"borrow.Incoming\", prog.finditer(\"borrow.Incoming\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence.split() はnltk.word_torknize(sentence) でも可\ndef tokenizer(sentence):\n    global text_preprocessing\n    if text_preprocessing:\n        return [word_compile(word) if len(prog.findall(word)) == 0 else splitter(word, prog.finditer(word))[0] \n                      for word in nltk.word_tokenize(sentence)]\n    else:\n        # どっかで数字だけの行があるぽいので，雑に str で処理しました\n        return sentence.split()\n\nsentences = [tokenizer(sentence) for sentence in all_document]\nif debug:\n    print(sentences[:2])\ndel all_document","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 出現回数の少ない順にソート（確認のため）\nword_count = defaultdict(lambda: 0)\nif text_preprocessing:\n    for sentence in sentences:\n        for word in sentence:\n            word_count[word] += 1\n    word_count = dict(sorted(word_count.items(), key = lambda kv: kv[1]))\nif debug:\n    print(word_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# word embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 面倒だったので､やっつけ仕事\nclass emb_model(object):\n    def __init__(self, vectors, index2word):\n        self.vectors = vectors\n        self.vector_size = vectors.shape[1]\n        self.index2word = index2word\n\nencoding = \"utf8\"\n#def float_decode(value):\n#    return float(value.decode(encoding))\n\ndef load_glove(file_path, word_candidates):\n    vectors = []\n    index2word = []\n    with open(file_path, \"rb\") as f:\n        for line in f:\n            #print(line)\n            sline = line.split()\n            word, vector = sline[0].decode(encoding), np.asarray(sline[1:], dtype=np.float32)\n            if vector.shape[0] != 300:\n                print(line)\n                break\n            if word_candidates is None or word in word_candidates:\n                vectors.append(vector)\n                index2word.append(word)\n    model = emb_model(np.asarray(vectors, dtype=np.float32), index2word)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if train_word2vec:\n    word2vec = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=MIN_COUNT , iter=5, negative=5)\nelse:\n    # word2vec でも学習したコーパスによって特性が変わる恐れがある\n    # これは Google News を学習したモデル．今回のテキストを学習したモデルも誰かがあげてくれているみたいです．\n    # 有名な単語分散表現学習モデルは word2vec, glove, fasttext\n    # 理解としては，word2vec がノーマル．glove がドメイン特化，fasttext が未知語に強いくらいです\n    # fasttext は未知語に強いですが，pre や con などの接頭辞・接尾辞にように同じ特徴を持つ場合に強いので\n    # FUUUUUUU などは確定で強いかはやってみてという感じ\n    # gloveとfasttextとcharngram は torch から直接呼び出せます\n    \n    word_candidates = set([word for sentence in sentences for word in sentence])\n    # 正直やりたくないが，メモリのためには仕方ない\n    del sentences\n    \n    # model なら word2vec = gensim.models.Word2Vec.load(\"\")　今回は bin 形式なので\n    word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n        os.path.join(input_dir, \"word2vec-google\", \"GoogleNews-vectors-negative300.bin\"), binary = True)\n    # 変数名はすいません・・・\n    # word2vec = load_glove(os.path.join(input_dir, \"glove840b300dtxt\", \"glove.840B.300d.txt\"), word_candidates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# emb_model は非対応\nif not isinstance(word2vec, emb_model):\n    print(\"check original index\", word2vec.vocab[\"suck\"].index)\n    if not train_word2vec:\n        # 拾ってきた分散表現は基本的に無駄にメモリを取るので，削ります　※バグを生みやすいのでお勧めはしません\n        # とりあえず，今回は雑に削ります．\n        index_candidates = [i for i, word in enumerate(word2vec.index2word) if word in word_candidates]\n        word2vec.vectors = word2vec.vectors[index_candidates]\n        word2vec.index2word = [word2vec.index2word[i] for i in index_candidates]\n\n        vocab_dict = {}\n        for i, word in enumerate(word2vec.index2word):\n            vocab = word2vec.vocab[word]\n            vocab.index = i\n            vocab_dict[word] = vocab\n        del word_candidates\n    print(\"check revise index\", word2vec.vocab[\"suck\"].index)\n    \n    if debug:\n        print(word2vec.most_similar(\"suck\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# データの用意\nすいませんが，validation までを用意します．\n予測までは追々\n\nメモリに既に展開済みかで対応を分けています"},{"metadata":{"trusted":true},"cell_type":"code","source":"if train_word2vec:\n    TEXT = torchtext.data.Field(sequential=True, include_lengths=False,\n                               batch_first=True, tokenize=tokenizer)\n    # 見た感じ，2値分類にしている例が多いので\n    TARGET = torchtext.data.LabelField(sequential=False, use_vocab=False,\n                                      batch_first=True, is_target=True, \n                             preprocessing=lambda x: int(float(x) >= 0.5))\n    fields=[('text', TEXT), ('target', TARGET)]\n\n    # 今回はリストに既に展開しているので Example.fromlist と Dataset\n    examples = [torchtext.data.Example.fromlist(\n                [sentence, target], fields=fields) \n                for sentence, target in zip(sentences, target)]\n\n    train_data = torchtext.data.Dataset(\n         examples, fields=fields)\n        \nelse:\n    TEXT = torchtext.data.Field(sequential=True, include_lengths=False,\n                               batch_first=True, tokenize=tokenizer)\n    # 見た感じ，2値分類にしている例が多いので\n    TARGET = torchtext.data.LabelField(sequential=False, use_vocab=False,\n                                      batch_first=True, is_target=True, \n                             preprocessing=lambda x: int(float(x) >= 0.5))\n    fields={\"comment_text\":('text', TEXT), \n            \"target\":('target', TARGET)}\n    \n    train_data = torchtext.data.TabularDataset(\n        path = os.path.join(input_dir, jigsaw_path, \"train.csv\"),\n        format=\"csv\",     \n        fields=fields,\n        skip_header=False)\n    \nif debug:\n    print(train_data.examples[0].text)\n    print(train_data.examples[0].target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 面倒なので新しいのを作りました\n# 普通は vectors から読み込めます\nclass word_vectors(torchtext.vocab.Vectors):\n    def __init__(self, model, unk_init=None):\n        self.itos = model.index2word\n        self.stoi = {word:i for i, word in enumerate(model.index2word)}\n        self.vectors = torch.FloatTensor(model.vectors)\n        self.dim = model.vector_size\n        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n        # self.cache(name, cache, url=url, max_vectors=max_vectors)\n# 今回の word2vec 用のベクトルを torch で使えるようにしてます        \nvectors = word_vectors(word2vec)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 単語を index 形式にして，出現頻度が低い単語をまとめてます\nTEXT.build_vocab(train_data, min_freq=MIN_COUNT, vectors=vectors)\nif debug:\n    print(TEXT.vocab.freqs.most_common(100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if debug:\n    print(TEXT.vocab.itos[:100])\n    # 元の値は 0.89 なので 2値化できているのを確認\n    print(train_data.examples[4].target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit の input データも用意\nbatch_test_size = 1024\nsubmit_dataset = torchtext.data.TabularDataset(os.path.join(input_dir, jigsaw_path, \"test.csv\"),\n                                            format='csv',\n                                            fields={'comment_text': ('text', TEXT)})\nsubmit_loader = torchtext.data.Iterator(submit_dataset, batch_size=batch_test_size,\n                                      device='cuda', shuffle=False, sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trainer の用意\nここは､どのような学習手法でも変わらないことが多いため使いまわせるものを自前で持っていた方が良い\n\n今回は高級なものを使用\n（こういったパッケージを使うと細かいところに手が届かなかったり､バージョン更新が面倒だったりする）\n\nその結果として，逆にコードが増えた．非常によろしくないと思う．"},{"metadata":{},"cell_type":"markdown","source":"# Trainer のイメージ\n普通に省略しすぎて，Train のイメージすら消してしまったので・・・\n\n位置は前後して良いですが､今回の場合\n\n- model の指定\n- optimizer の指定\n\n- STARTED    \n    - EPOCH STARTED\n    \n        cross validation によるデータの指定\n        \n        - ITERATION STARTED\n            \n            process_function（train に対する学習）            \n            \n        - ITERATION COMPLETED\n        \n   eval_function（train, validation 等の評価）\n        \n    - EPOCH COMPLETED\n- COMPLETED\n\nget_prediction（submit 用の予測）"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 誤差関数の指定\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習時\ndef process_function(engine, batch):\n    # 学習用\n    model.train()\n    # 勾配の初期化\n    optimizer.zero_grad()\n    # データの読み込み\n    x, y = batch.text.cuda(), batch.target.cuda()\n    # 順伝播\n    y_pred = model(x)\n    # 誤差の算出\n    loss = criterion(y_pred, y)\n    # 勾配の計算\n    loss.backward()\n    # パラメータの更新\n    optimizer.step()\n    return loss.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 評価時\ndef eval_function(engine, batch):\n    # 評価用（一部の層では学習時と評価時で動作が変わるため）\n    model.eval()\n    with torch.no_grad():\n        x, y = batch.text.cuda(), batch.target.cuda()\n        y_pred = model(x)\n        return y_pred, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測用\n# 予測用では，引数がmodel であったり\n# ソフトマックスにかけている所が違います\ndef get_predictions(model, loader):\n    model.eval()\n    with torch.no_grad():\n        predictions = []\n        for batch in loader:\n            x = batch.text.cuda()\n            logits = model(x)\n            y_pred = F.softmax(logits, dim=1)[:, 1]\n            # move from GPU to CPU and convert to numpy array\n            # cpu で計算できる方にする必要があります\n            y_pred_numpy = y_pred.cpu().numpy()\n\n            predictions.append(y_pred_numpy)\n        predictions = np.concatenate(predictions)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 今回はignite の Engine を使ったので､学習部分と評価部分を定めます\ntrainer = Engine(process_function)\ntrain_evaluator = Engine(eval_function)\nvalidation_evaluator = Engine(eval_function)\ntest_evaluator = Engine(eval_function)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ここら辺が逆に不便なのですが､metrics を追加していきます\n# 追加method.attach(追加engine, metric名)\nRunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')\n\n# 小数点をカットするだけなので､なくても可\ndef thresholded_output_transform(output):\n    y_pred, y = output\n    y_pred = torch.round(y_pred)\n    return y_pred, y\n# accurracy, loss の追加\nAccuracy(output_transform=thresholded_output_transform).attach(\n    train_evaluator, 'accuracy')\nLoss(criterion).attach(train_evaluator, 'ce')\nAccuracy(output_transform=thresholded_output_transform).attach(\n    validation_evaluator, 'accuracy')\nLoss(criterion).attach(validation_evaluator, 'ce')\nAccuracy(output_transform=thresholded_output_transform).attach(\n    test_evaluator, 'accuracy')\nLoss(criterion).attach(test_evaluator, 'ce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 内部で新しい tqdm が入れられるが，自前で定義済みなので\n# 定義してなければ不要\nclass CustomProgressBar(ProgressBar):\n    def __init__(self, tqdm, persist=False,\n                 bar_format='{desc}[{n_fmt}/{total_fmt}] {percentage:3.0f}%|{bar}{postfix} [{elapsed}<{remaining}]',\n                 **tqdm_kwargs):\n        self.pbar_cls = tqdm\n        self.pbar = None\n        self.persist = persist\n        self.bar_format = bar_format\n        self.tqdm_kwargs = tqdm_kwargs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習時に表示するものやログに残すものを指定します\n\n# tensorboard 用の logger\n# 必要な metrics を指定してます\ntb_logger = TensorboardLogger(log_dir=log_path)\ntb_logger.attach(trainer,\n                 log_handler=OutputHandler(tag=\"training\", output_transform=lambda loss: {'loss': loss}),\n                 event_name=Events.ITERATION_COMPLETED)\ntb_logger.attach(train_evaluator,\n                 log_handler=OutputHandler(tag=\"training\",\n                                              metric_names=[\"ce\", \"accuracy\"],\n                                              another_engine=trainer),\n                 event_name=Events.EPOCH_COMPLETED)\ntb_logger.attach(validation_evaluator,\n                 log_handler=OutputHandler(tag=\"validation\",\n                                           metric_names=[\"ce\", \"accuracy\"],\n                                           another_engine=trainer),\n                 event_name=Events.EPOCH_COMPLETED)\ntb_logger.attach(test_evaluator,\n                 log_handler=OutputHandler(tag=\"test\",\n                                           metric_names=[\"ce\", \"accuracy\"],\n                                           another_engine=trainer),\n                 event_name=Events.EPOCH_COMPLETED)\n\n# tqdm 用の logger \n# submit では使えない\nif is_interactive():\n    pbar = CustomProgressBar(tqdm=tqdm, persist=True, bar_format=\"\")\n    pbar.attach(trainer, ['loss'])\n    \ndef plot_metrics(engine, metrics):\n    if is_interactive():\n        pbar.log_message(\n            \"Training Results - Epoch: {} \\nMetrics\\n{}\"\n            .format(engine.state.epoch, pprint.pformat(metrics)))\n    else:\n        print(\"Training Results - Epoch: {} \\nMetrics\\n{}\"\n            .format(engine.state.epoch, pprint.pformat(metrics)))\n    \ndef log_training_results(engine):\n    train_evaluator.run(train_loader)\n    metrics = train_evaluator.state.metrics\n    plot_metrics(engine, metrics)\n\ndef log_validation_results(engine):\n    validation_evaluator.run(val_loader)\n    metrics = validation_evaluator.state.metrics\n    plot_metrics(engine, metrics)\n        \ndef log_test_results(engine):\n    test_evaluator.run(test_loader)\n    metrics = test_evaluator.state.metrics\n    plot_metrics(engine, metrics)\n\ntrainer.add_event_handler(Events.EPOCH_COMPLETED, log_training_results)\ntrainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\ntrainer.add_event_handler(Events.EPOCH_COMPLETED, log_test_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 正直，いらなくね．と思いましたが，一応残しときます\ndef score_function(engine):\n    val_loss = engine.state.metrics['ce']\n    return -val_loss\n\n# score function によってモデルを評価してます（loss の負の値なので，lossの小さい順）\n# patient は loss が何回（連続？）で良くならなかったかを我慢する数\nhandler = EarlyStopping(patience=3, score_function=score_function, trainer=trainer)\n\ntest_evaluator.add_event_handler(Events.COMPLETED, handler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best モデルの save\nbest_model_save = ModelCheckpoint(\n    best_model_path, 'BiLSTM', n_saved=1,\n    create_dir=True, save_as_state_dict=True,\n    score_function=score_function)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# cross validation\n2重のバリデーション（nested cross validation?）になっている\n\n### これだと，データ全部を使うわけではないので注意"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1024\ntest_size = 0.1\n# kfold の分割数\nn_folds = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torchtext は普通の index で回すのに向いてないので（不便すぎる）\n# kflold に合わせるための変更\nclass TorchtextSubset(Subset):\n    def __init__(self, dataset, indices):\n        super(TorchtextSubset, self).__init__(dataset, indices)\n        self.fields = self.dataset.fields\n        self.sort_key = self.dataset.sort_key","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\nlabels = [example.target for example in train_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(range(len(train_data)), labels, test_size=test_size, random_state=SEED)\nlen(X_train), len(X_test), len(y_train), len(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_splits = list(skf.split(X_train, y=y_train))\nif debug:\n    print(len(idx_splits))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation にするために，epoch 毎に index を変えている\n@trainer.on(Events.EPOCH_STARTED)\ndef cross_validation(engine):\n    # loader も global 変数で回るので global 変数にしなければいけない（不便）\n    global train_loader, val_loader\n    train_idx, val_idx = idx_splits[engine.state.epoch-1 % n_folds]\n    train_ds = TorchtextSubset(train_data, train_idx)\n    val_ds = TorchtextSubset(train_data, val_idx)\n\n    # データ長を揃えてくれる（sort key で指定）\n    train_loader, val_loader = torchtext.data.BucketIterator.splits(\n        [train_ds, val_ds], batch_sizes=[batch_size, batch_size], device=device,\n        sort_key=lambda x: len(x.text),\n        sort_within_batch=True, repeat=False)\n    engine.state.dataloader = train_loader\n    \ntest_idx = X_test\ntest_ds = TorchtextSubset(train_data, test_idx)\ntest_loader = torchtext.data.BucketIterator(dataset=test_ds, batch_size=batch_size,\n                                            device=device, sort_key=lambda x: len(x.text),\n                                            sort_within_batch=True, repeat=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# model の用意\nちょっと時間的にすぐ終わらなそうなので，モデルは借りました\nhttps://www.kaggle.com/protan/lstm-cnn-torchtext-with-ignite\n\n※ただし，アンサンブルと target 以外の予測は簡単化のため消してます\n\n時系列のモデルとしては，RNN, LSTM, GRU が有名です．\n※ Transformer というモデルが言語処理では最も強いと言われていますが，上記と並べにくいので省いています\n- RNN は simple な時系列に対応したネットワークで，時系列長が長いと学習に失敗しやすいです\n- LSTM は Long Short Term Memory の略称で時系列で離れた単語や近い単語を強調したりする仕組みが追加されています\n- GRU は LSTM の枠組みをシンプルにしたもので，ネットワークのパラメータが LSTM より少なく時系列長に対して RNN より頑強です\n\nRNN と LSTM\nhttps://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca\n\nLSTMのみならこれが一番だと思います\nhttps://qiita.com/t_Signull/items/21b82be280b46f467d1b\n\nGRU はこれらのモデルがわかっていれば概要図か数式を見ればすぐわかるかと思います\n\n純粋な拡張として，attention+RNN があります\n\nその他にも kernel で見られるように単語分散表現以外にも単語のベクトルが得られる手法があり，それらが，ELMO, open-NMT, BERT というもので pretrained のものを興味があれば使えば良いと思います．\n（学習時間的に基本無理です）"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習用パラメーター\n# CNN 用\nkernel_sizes = [3, 4, 5]\nnum_filters = 64\n# クラス数\nnum_classes = 2\n# embedding の学習の有無\nmode = 'nonstatic' #'static' or 'nonstatic'\nhidden_dim = 256\nlstm_units = 128\n# spatial dropout をする確率\nspatial_drop = 0.1\n# 通常の dropout をする確率\nd_prob = 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropout は BatchNormalization 同様有名です\n# Dropout は確率で選択した一部のノードを使用しない方法で過学習を避けやすい\n# らしいです．元の論文を確かめてないです．\n# Spatial は Dropout に比べて，連続的に落とします\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass BiLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters,\n                 num_classes, d_prob, mode, hidden_dim, lstm_units,\n                 emb_vectors=None, spatial_drop=0.1):\n        super(BiLSTM, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        #self.kernel_sizes = kernel_sizes\n        #self.num_filters = num_filters\n        self.num_classes = num_classes\n        self.d_prob = d_prob\n        self.mode = mode\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n        self.embedding_dropout = SpatialDropout(spatial_drop)\n        \n        # embedding の読み込みと学習するかどうかの確認\n        if emb_vectors is not None:\n            self.load_embeddings(emb_vectors)\n        # CNN は絶対じゃないので\n        \"\"\"\n        self.conv = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim,\n                                             out_channels=num_filters,\n                                             kernel_size=k, stride=1) for k in kernel_sizes])\n        \"\"\"\n        # 1個目のBiLSTM\n        self.lstm1 = nn.LSTM(embedding_dim, lstm_units,\n                             bidirectional=True, batch_first=True)\n        # 2個目のLSTM\n        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units,\n                             bidirectional=True, batch_first=True)\n        #self.lstm_body = nn.LSTM(\n        #    embedding_dim, lstm_units, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(d_prob)\n        # remove CNN\n        #self.fc = nn.Linear(len(kernel_sizes) * num_filters, hidden_dim)\n        #self.fc_total = nn.Linear(hidden_dim * 1 + lstm_units * 4, hidden_dim)\n        self.fc_total = nn.Linear(lstm_units * 4, hidden_dim)\n        self.fc_final = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        x_emb = self.embedding(x)\n        x_emb = self.embedding_dropout(x_emb)\n        \n        # remove CNN\n        # pad for CNN kernel 5\n        \"\"\"\n        if x_emb.shape[1] < 5:\n            x_emb = F.pad(x_emb, (0, 0, 0, 5 - x_emb.shape[1]), value=0)\n        \n        x = [F.relu(conv(x_emb.transpose(1, 2))) for conv in self.conv]\n        x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n        x = torch.cat(x, dim=1)\n        x = self.fc(self.dropout(x))\n        \"\"\"\n\n        h_lstm1, _ = self.lstm1(x_emb)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n\n        # average pooling\n        avg_pool2 = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool2, _ = torch.max(h_lstm2, 1)\n        \n        # cat は concat（結合）です\n        #out = torch.cat([x, avg_pool2, max_pool2], dim=1)\n        out = torch.cat([avg_pool2, max_pool2], dim=1)\n        # relu は活性化関数の一つです．これもいくつか種類があります\n        out = F.relu(self.fc_total(self.dropout(out)))\n        out = self.fc_final(out)\n\n        return out\n\n    # 参考にした人が丁寧な人だったので\n    # embedding 層を学習するかしないかを選べるようにしてくれています\n    # non static では最初に入れた embedding の重みも変わります\n    # 転移学習したい時などに embedding を学習させます\n    def load_embeddings(self, emb_vectors):\n        if 'static' in self.mode:\n            self.embedding.weight.data.copy_(emb_vectors)\n            if 'non' not in self.mode:\n                self.embedding.weight.data.requires_grad = False\n                print('Loaded pretrained embeddings, weights are not trainable.')\n            else:\n                self.embedding.weight.data.requires_grad = True\n                print('Loaded pretrained embeddings, weights are trainable.')\n        elif self.mode == 'rand':\n            print('Randomly initialized embeddings are used.')\n        else:\n            raise ValueError(\n                'Unexpected value of mode. Please choose from static, nonstatic, rand.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# もっと高レベルなモデルの可視化の方法もあるので､調べるとよいと思います\n# 今回は､最低限の確認です\nvocab_size, embedding_dim = TEXT.vocab.vectors.shape\nmodel = BiLSTM(vocab_size=vocab_size,\n              embedding_dim=embedding_dim,\n              kernel_sizes=kernel_sizes,\n              num_filters=num_filters,\n              num_classes=num_classes,\n              d_prob=d_prob,\n              mode=mode,\n              hidden_dim=hidden_dim,\n              lstm_units=lstm_units,\n              spatial_drop=spatial_drop,\n              emb_vectors=TEXT.vocab.vectors)\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model や optimizer は global 変数で共有しているよう（気持ち悪いけども）\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ntest_evaluator.add_event_handler(Events.EPOCH_COMPLETED, best_model_save, {'text_model': model})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.run(None, max_epochs=10)\n\n# tensorboard logger は閉じる必要がある\ntb_logger.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# log の確認"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 一応，ログも出しておいた\nlog = {\"loss\": {\"training\":[]},\n       \"accuracy\": {\"training\":[], \"validation\":[], \"test\":[]},\n       \"ce\": {\"training\":[], \"validation\":[], \"test\":[]}}\nfor event in tf.train.summary_iterator(os.path.join(log_path, os.listdir(log_path)[0])):\n    for value in event.summary.value:\n        if value.HasField('simple_value'):\n            timing, metric = value.tag.split(\"/\")\n            log[metric][timing].append(value.simple_value)\n            \nfor key, value in log.items():\n    for k, v in value.items():\n        plt.plot(np.arange(len(log[key][k])), log[key][k], label=k)\n    plt.title(key)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 提出時は使えない\n%tensorboard --logdir experiments/tb_logs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit データに対する予測"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load best model \nmodel_path = os.path.join(best_model_path, os.listdir(best_model_path)[0])\nmodel_state_dict = torch.load(model_path)\nmodel.load_state_dict(model_state_dict)\n\npredictions = get_predictions(model, submit_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit ファイルの作成"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 提出ファイルを作成\ndf_sub = pd.read_csv(os.path.join(input_dir, jigsaw_path, \"sample_submission.csv\"))\ndf_sub['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}