{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "from spacy.lang.en import English\n",
    "import chainer\n",
    "from chainer import datasets\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "from chainer.backends import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "prog = re.compile('\\.[A-Z]')\n",
    "Head_check = re.compile('^[A-Z][^A-Z]+$')\n",
    "nlp = English()\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "# test_df = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902194"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df) + len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1902194,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_document = pd.concat([train_df[\"comment_text\"],test_df[\"comment_text\"]])\n",
    "all_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 1.19 s, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = []\n",
    "for s in train_df['comment_text']:\n",
    "    t.append(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = [\",\", \".\", \"?\", \"!\", \"'\"]\n",
    "for i in range(len(t)):\n",
    "    for del_ele in del_list:\n",
    "        if del_ele in t[i]:\n",
    "            t[i].remove(del_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "for i in range(len(t)):\n",
    "    for word in t[i]:\n",
    "        if word not in words:\n",
    "            words[word] = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670966"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len = len(words)\n",
    "word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vec = []\n",
    "for i in range(len(t)):\n",
    "    sentence_ids = []\n",
    "    for word in t[i]:\n",
    "        sentence_ids.append(words[word])\n",
    "    sentence_ids = np.array(sentence_ids, dtype=\"int32\")\n",
    "    t_vec.append(sentence_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1804874"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(list(map(len, t_vec)))\n",
    "for i in range(len(t_vec)):\n",
    "    for j in range((max_len-len(t_vec[i]))):\n",
    "        t_vec[i] = np.append(t_vec[i], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, t_vec))\n",
    "max(list(map(len, t_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, window=5, min_count=MIN_COUNT , iter=5, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 100)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.wv[sentences[0]]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('screw', 0.7846056818962097),\n",
       " ('soak', 0.7200046181678772),\n",
       " ('blow', 0.71183180809021),\n",
       " ('gobble', 0.6828003525733948),\n",
       " ('sucking', 0.6570413708686829),\n",
       " ('screwed', 0.6553224921226501),\n",
       " ('fucked', 0.6482899188995361),\n",
       " ('wake', 0.6468736529350281),\n",
       " ('lighten', 0.6461282968521118),\n",
       " ('snuff', 0.6439306735992432)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('suck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ohara_lab/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save(\"jigsaw_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ohara_lab/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(\"jigsaw_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 100)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[sentences[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jigsaw_dataset(chainer.dataset.DatasetMixin):\n",
    "    def __init__(self, train=True, min_count=10, train_size=0.6):\n",
    "        train_df = pd.read_csv(\"../input/train.csv\")\n",
    "        \n",
    "        t = []\n",
    "        for s in train_df['comment_text']:\n",
    "            t.append(s.split())\n",
    "                \n",
    "        words = {}\n",
    "        for j in range(len(t)):\n",
    "            for word in t[j]:\n",
    "                if word not in words:\n",
    "                    words[word] = len(words)\n",
    "        \n",
    "        t_vec = []\n",
    "        for j in range(len(t)):\n",
    "            t_ids = []\n",
    "            for word in t[j]:\n",
    "                t_ids.append(words[word])\n",
    "            t_ids = np.array(t_ids, dtype=np.int32)\n",
    "            t_vec.append(t_ids)\n",
    "        \n",
    "        max_len = max(list(map(len, t_vec)))\n",
    "        for i in range(len(t_vec)):\n",
    "            for j in range((max_len-len(t_vec[i]))):\n",
    "                t_vec[i] = np.append(t_vec[i], -1)\n",
    "        \n",
    "        x = np.array(t_vec, dtype=np.int32)\n",
    "        \n",
    "        label= np.where(train_df['target']>0.5, 1, 0)\n",
    "        label = pd.get_dummies(label) # comment out\n",
    "        y = label.values\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(x, y, train_size=train_size, random_state=1)\n",
    "        self.n_train = len(self.y_train)\n",
    "        self.n_test = len(self.y_test)\n",
    "        self.train = train\n",
    "        self.train_size=train_size\n",
    "        \n",
    "        del t, words, t_vec, t_ids, x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.n_train\n",
    "        else:\n",
    "            return self.n_test\n",
    "    \n",
    "    def get_example(self, i):\n",
    "        \n",
    "        train = datasets.tuple_dataset.TupleDataset(self.X_train, self.y_train)\n",
    "        test = datasets.tuple_dataset.TupleDataset(self.X_test, self.y_test)\n",
    "        \n",
    "        if self.train:\n",
    "            return train[i]\n",
    "        else:\n",
    "            return test[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7fd6cab30be0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(chainer.Chain):\n",
    "    def __init__(self, n_vocab=1670966, n_units=100):\n",
    "        super(RNN, self).__init__(\n",
    "            embed = L.EmbedID(n_vocab, n_units),\n",
    "            l1 = L.LSTM(None, n_units),\n",
    "            l2 = L.LSTM(None, n_units),\n",
    "            l3 = L.Linear(None, 2)\n",
    "        )\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0))\n",
    "        h2 = self.l2(F.dropout(h1))\n",
    "        y = F.softmax(self.l3(h2))\n",
    "        return y\n",
    "\n",
    "\n",
    "model = RNN()\n",
    "\n",
    "# gpu_id = 0\n",
    "# if gpu_id >= 0:\n",
    "#     model.to_gpu(gpu_id)\n",
    "    \n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "MAX_EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ohara_lab/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 40s, sys: 7.92 s, total: 40min 48s\n",
      "Wall time: 40min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = jigsaw_dataset(train_size=0.1)\n",
    "test = jigsaw_dataset(train_size=0.1)\n",
    "train_iter = chainer.iterators.SerialIterator(train, BATCH_SIZE)\n",
    "test_iter = chainer.iterators.SerialIterator(test, BATCH_SIZE, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.4846383  0.5153617 ]\n",
      "          [0.33302444 0.6669756 ]\n",
      "          [0.38394952 0.6160505 ]\n",
      "          [0.39510062 0.6048994 ]\n",
      "          [0.49786264 0.50213736]])\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    },
    {
     "ename": "InvalidType",
     "evalue": "\nInvalid operation is performed in: SoftmaxCrossEntropy (Forward)\n\nExpect: t.ndim == x.ndim - 1\nActual: 2 != 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidType\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-228bf9599578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/chainer/functions/loss/softmax_cross_entropy.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(x, t, normalize, cache_score, class_weight, ignore_label, reduce, enable_double_backprop)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_chainerx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_chainerx_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mhooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36m_check_data_type_forward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0min_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in_types'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function_check_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/chainer/functions/loss/softmax_cross_entropy.py\u001b[0m in \u001b[0;36mcheck_type_forward\u001b[0;34m(self, in_types)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mx_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mx_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/chainer/utils/type_check.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(*bool_exprs)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexpr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbool_exprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/chainer/utils/type_check.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m             raise InvalidType(\n\u001b[1;32m    482\u001b[0m                 \u001b[0;34m'{0} {1} {2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 '{0} {1} {2}'.format(left, self.inv, right))\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidType\u001b[0m: \nInvalid operation is performed in: SoftmaxCrossEntropy (Forward)\n\nExpect: t.ndim == x.ndim - 1\nActual: 2 != 1"
     ]
    }
   ],
   "source": [
    "while train_iter.epoch < MAX_EPOCH:\n",
    "    train_batch = train_iter.next()\n",
    "    sentence_train, target_train = chainer.dataset.concat_examples(train_batch)\n",
    "#     sentence_train, target_train = chainer.dataset.concat_examples(train_batch, gpu_id)\n",
    "    \n",
    "    # target_train = [0, 1, ,1 , 0 , 1, 0,...]\n",
    "    prediction_train = model(sentence_train)\n",
    "    # prediction_trian = [(0.5, 0.5), (0.2, 0.8)...]\n",
    "    loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update()\n",
    "    if train_iter.is_new_epoch:\n",
    "        print('epoch:{:02d} train_loss:{:.04f}'.format(train_iter.epoch, float(loss.array)), end='')\n",
    "        \n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            sentence_test, target_test = chainer.dataset.concat_examples(test_batch)\n",
    "#             sentence_test, target_test = chainer.dataset.concat_examples(test_batch, gpu_id)\n",
    "            \n",
    "            prediction_test = model(sentence_test)\n",
    "            loss_test = F.mean_squared_error(prediction_test, target_test)\n",
    "            test_losses.append(loss_test.array)\n",
    "#             test_losses.append(cuda.to_cpu(loss_test.array))\n",
    "            \n",
    "            accuracy = F.accuracy(prediction_test, target_test)\n",
    "#             accuracy.cuda.to_cpu()\n",
    "            test_accuracies.append(accuracy.array)\n",
    "            \n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch = 0\n",
    "                test_iter.current_position = 0\n",
    "                test_iter.is_new_epoch = False\n",
    "                test_iter._pushed_position = None\n",
    "                break;\n",
    "        \n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(np.mean(test_losses), np.mean(test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
