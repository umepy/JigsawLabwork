{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import datasets\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "from chainer.backends import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.cupy\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "head_check = re.compile('^[A-Z][^A-Z]+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCH = 10\n",
    "# length of truncated BPTT\n",
    "BPROP_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "all_df = pd.concat([train_df['comment_text'], test_df['comment_text']])\n",
    "\n",
    "y= np.where(train_df['target']>0.5, 1, 0)\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "t_all = []\n",
    "for s in all_df.values:\n",
    "    t_all.append(nltk.word_tokenize(s))\n",
    "\n",
    "del all_df\n",
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_all_sub = []\n",
    "for i in range(len(t_all)):\n",
    "    vector = []\n",
    "    for t in t_all[i]:\n",
    "        check = head_check.match(t)\n",
    "        if check is not None:\n",
    "            add = t[0].lower() + t[1:]\n",
    "        else:\n",
    "            add = t\n",
    "        vector.append(add)\n",
    "    t_all_sub.append(vector)\n",
    "\n",
    "t_all = []\n",
    "for i in range(len(t_all_sub)):\n",
    "    vector = []\n",
    "    for t in t_all_sub[i]:\n",
    "        vector.append(wnl.lemmatize(t))\n",
    "    t_all.append(vector)\n",
    "\n",
    "del t_all_sub\n",
    "del vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = []\n",
    "t_pre = []\n",
    "for i in range(len(y)):\n",
    "    t_train.append(t_all[i])\n",
    "for i in range(len(y),len(t_all)):\n",
    "    t_pre.append(t_all[i])\n",
    "\n",
    "# words = {}\n",
    "# for j in range(len(t_all)):\n",
    "#     for word in t_all[j]:\n",
    "#         if word not in words:\n",
    "#             words[word] = len(words)+1\n",
    "\n",
    "word2vec = gensim.models.Word2Vec(t_all, size=300, window=5, min_count=MIN_COUNT , iter=5, negative=5)\n",
    "\n",
    "i2w = word2vec.wv.index2word\n",
    "words = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "del t_all\n",
    "word_length = len(word2vec.wv.index2word)\n",
    "\n",
    "X = []\n",
    "for j in range(len(t_train)):\n",
    "    vector = []\n",
    "    for word in t_train[j]:\n",
    "        vector.append(words[word])\n",
    "    vector = np.array(vector, dtype=np.int32)\n",
    "    X.append(vector)\n",
    "\n",
    "X_pre = []\n",
    "for j in range(len(t_pre)):\n",
    "    vector = []\n",
    "    for word in t_pre[j]:\n",
    "        vector.append(words[word])\n",
    "    vector = np.array(vector, dtype=np.int32)\n",
    "    X_pre.append(vector)\n",
    "\n",
    "del t_train\n",
    "del t_pre\n",
    "del words\n",
    "del vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train = max(list(map(len, X)))\n",
    "max_len_test = max(list(map(len, X_pre)))\n",
    "max_len = max_len_train if max_len_train>max_len_test else max_len_test\n",
    "del max_len_train\n",
    "del max_len_test\n",
    "\n",
    "for i in range(len(X)):\n",
    "    add = np.zeros((max_len-len(X[i])), dtype=np.int32)\n",
    "    X[i] = np.append(X[i], add)\n",
    "\n",
    "for i in range(len(X_pre)):\n",
    "    add = np.zeros((max_len-len(X_pre[i])), dtype=xp.int32)\n",
    "    X_pre[i] = np.append(X_pre[i], add)\n",
    "\n",
    "del add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jigsaw_dataset(chainer.dataset.DatasetMixin):\n",
    "    def __init__(self, X, y, train=True, train_size=0.6):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, train_size=train_size, random_state=1)\n",
    "        self.n_train = len(self.y_train)\n",
    "        self.n_test = len(self.y_test)\n",
    "        self.train = train\n",
    "        self.train_size=train_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.n_train\n",
    "        else:\n",
    "            return self.n_test\n",
    "    \n",
    "    def get_example(self, i):\n",
    "        if self.train:\n",
    "            train = datasets.tuple_dataset.TupleDataset(self.X_train, self.y_train)\n",
    "            return train[i]\n",
    "        else:\n",
    "            test = datasets.tuple_dataset.TupleDataset(self.X_test, self.y_test)\n",
    "            return test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(chainer.Chain):\n",
    "    def __init__(self, n_vocab=73609, n_units=300, w=None):\n",
    "        super(RNN, self).__init__(\n",
    "            embed = L.EmbedID(n_vocab, n_units, initialW=w),\n",
    "            l1 = L.LSTM(None, n_units),\n",
    "            l2 = L.LSTM(None, n_units),\n",
    "            l3 = L.Linear(None, 2)\n",
    "        )\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0))\n",
    "        h2 = self.l2(F.dropout(h1))\n",
    "        y = F.softmax(self.l3(h2))\n",
    "        return y\n",
    "\n",
    "\n",
    "model = L.Classifier(RNN(n_vocab=word2vec.wv.vectors.shape[0], n_units=word2vec.wv.vectors.shape[1], w=word2vec.wv.vectors))\n",
    "\n",
    "gpu_id = 0\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)\n",
    "    \n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = jigsaw_dataset(X, y)\n",
    "test = jigsaw_dataset(X, y, train=False)\n",
    "del X\n",
    "del y\n",
    "train_iter = chainer.iterators.SerialIterator(train, BATCH_SIZE)\n",
    "test_iter = chainer.iterators.SerialIterator(test, BATCH_SIZE, repeat=False, shuffle=False)\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iter, gpu_id):\n",
    "    evaluator = model.copy()\n",
    "    evaluator.predictor.reset_state()\n",
    "    sum_perp = 0\n",
    "    data_count = 0\n",
    "    with chainer.configuration.using_config('train', False):\n",
    "        with chainer.using_config('enable_backprop', False):\n",
    "            iter.reset()\n",
    "            for batch in iter:\n",
    "                sentence, target = chainer.dataset.convert.concat_examples(batch, gpu_id)\n",
    "                loss = evaluator(sentence, target)\n",
    "                sum_perp += loss.array\n",
    "                data_count += 1\n",
    "    return np.exp(float(sum_perp) / data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_perp = 0\n",
    "count = 0\n",
    "iteration = 0\n",
    "while train_iter.epoch < MAX_EPOCH:\n",
    "        loss = 0\n",
    "        iteration += 1\n",
    "        for i in range(BPROP_LEN):\n",
    "            train_batch = train_iter.__next__()\n",
    "            sentence_train, target_train = chainer.dataset.convert.concat_examples(train_batch, gpu_id)\n",
    "            loss += optimizer.target(sentence_train, target_train)\n",
    "            if train_iter.is_new_epoch:\n",
    "                break\n",
    "        count += 1\n",
    "        sum_perp += loss.array\n",
    "        optimizer.target.cleargrads()\n",
    "        loss.backward()\n",
    "        loss.unchain_backward()\n",
    "        optimizer.update()\n",
    "        # 1082924\n",
    "        if train_iter.is_new_epoch:\n",
    "            print('epoch:{}'.format(train_iter.epoch))\n",
    "            print('test perplexity:{}'.format(evaluate(model, test_iter, gpu_id)))\n",
    "del train_iter\n",
    "del test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model, X_pre):\n",
    "    prediction_cpu = []\n",
    "    with chainer.configuration.using_config('train', False):\n",
    "        with chainer.using_config('enable_backprop', False):\n",
    "            for i in range(len(X_pre)):\n",
    "#                 print(i)\n",
    "                p = xp.array([X_pre[i]])\n",
    "                prediction = model.predictor(p)\n",
    "                del p\n",
    "                p_cpu = cuda.to_cpu(prediction.array)\n",
    "                del prediction\n",
    "                prediction_cpu.append(p_cpu)\n",
    "    return prediction_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cpu = predictor(model, X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "pre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(prediction_cpu)):\n",
    "    p = np.where(prediction_cpu[i][0][0] > prediction_cpu[i][0][1], 0, 1)\n",
    "    pre_df.loc[i, 'prediction'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df['prediction'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
